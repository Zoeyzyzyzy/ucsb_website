<html>
  <head>
    <meta charset="utf-8" />
    <title>Application Servers</title>
    <style>
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
        margin-bottom: 0;
      }
      .remark-slide-content h1 { font-size: 2.5em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      blockquote {
        border-left: 0.3em solid rgba(0,0,0,0.5);
        padding: 0 15px;
        font-style: italic;
      }
      code {
        background: #e7e8e2;
        border-radius: 5px;
      }
      img {
        max-height: 100%;
        max-width: 100%;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .clear { clear: both; }
      #slideshow .slide .content code { font-size: 0.8em; }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #272822;
        color: #777872;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 1em;
      }
      .background-blue {
        background-color: deepskyblue;
      }
      .background-green {
        background-color: springgreen;
      }
      .background-pink {
        background-color: deeppink;
      }

      /* Two-column layout */
      .left-column {
        width: 49%;
        float: left;
      }
      .right-column {
        width: 49%;
        float: right;
      }
      .left-column20 {
        width: 20%;
        float: left;
      }
      .right-column80 {
        width: 79%;
        float: right;
      }
      .left-column30 {
        width: 30%;
        float: left;
      }
      .right-column70 {
        width: 69%;
        float: right;
      }
      .left-column40 {
        width: 40%;
        float: left;
      }
      .right-column60 {
        width: 59%;
        float: right;
      }

      /* Table layout */
      table {
        background: #f5f5f5;
        margin: 30px auto;
        text-align: left;
        width: 100%;
      }
      th {
        background: linear-gradient(#777, #444);
        color: #fff;
        font-weight: bold;
        padding: 10px 15px;
      }
      td {
        border-right: 1px solid #fff;
        border-left: 1px solid #e8e8e8;
        border-top: 1px solid #fff;
        border-bottom: 1px solid #e8e8e8;
        padding: 10px 15px;
      }

    </style>
  </head>
  <body>
    <textarea id="source">
class: center, middle

# Application Servers

## CS291A: Scalable Internet Services

### Dr. Bryce Boe

---

# Separation of Responsibilities

> Why not use a single process to handle both the HTTP request and the
> application logic?

--

The concerns and design goals of HTTP servers are different from those of
application servers.

---

# Server Design Goals

## HTTP Server

* Provide a high performance HTTP implementation (handles concurrency)

* Be extremely stable, and relatively static

* Be very configurable and language/framework agnostic

--

## Application Server

* Support a specific language (e.g., Ruby), many of which are not optimized for
  performance

* Run _business logic_ which is extremely dynamic

---

# Application Servers

We are building web applications, so we will require complex server-side logic.

We _can_ extend our HTTP servers to provide this logic through modules, but
there are benefits to separating application servers into one ore more distinct
processes. Because:

--

* Application logic will be dynamic

* Application logic regularly uses high level (slow) languages

* Security concerns are easier (HTTP server can shield app server from
  malformed requests)

* Setup costs can be amortized if the app server is running continuously

---

# Application Server Architectures

> What architecture should we use for our application server?

--

We have the same trade-offs to consider as with HTTP servers (e.g. threads,
processes, and/or workers), so we needn't revisit them again.

---

class: center middle
# How does an HTTP Server communicate with the application server(s)?

---

# Inter-server Communication


## [CGI](https://en.wikipedia.org/wiki/Common_Gateway_Interface)

Spawn a process, pass HTTP headers as ENV variables and utilize STDOUT as the
response.

--

## [FastCGI](https://en.wikipedia.org/wiki/FastCGI), [SCGI](https://en.wikipedia.org/wiki/Simple_Common_Gateway_Interface)

Modifications to CGI to allow for persistent application server processes
(amortizes setup time).

--

## HTTP

Communicate via the HTTP protocol to a long-running process. (Essentially a
reverse-proxy configuration).

> Does it make sense to have the application server speak HTTP?

---

# Up Next

Let's take a quantitative look at various approaches used in actual Ruby
application servers.

We will not consider evented ruby application servers (e.g., EventMachine)
because Rails will not run on such application servers.

---

# Our Test Setup

![Demo App](demo_app.png)

The [Demo App](https://github.com/scalableinternetservices/demo) is a link
sharing website with:

* Multiple communities

* Each community can have many submissions

* Each submission can have a tree of comments

---

# Simulated Users

Using [Tsung](http://tsung.erlang-projects.org/) (erlang-based test framework)
we will simulate multiple users visiting the Demo App web service. Each user
will:

```
Visit the homepage (/)
  Wait randomly between 0 and 2 seconds
Request community creation form
  Wait randomly between 0 and 2 seconds
Submit new community form
Request new link submission form
  Wait randomly between 0 and 2 seconds
Submit new link submission form
  Wait randomly between 0 and 2 seconds
Delete the link
  Wait randomly between 0 and 2 seconds
Delete the community
```

---

# Test Process

There are six phases of testing each lasting 60 seconds:

1. (0-59s) Every second a new simulated user arrives

2. (60-119s) Every second 1.5 new simulated users arrive

3. (120-179s) Every second 2 new simulated users arrive

4. (180-239s) Every second 2.5 new simulated users arrive

5. (240-299s) Every second 3 new simulated users arrive

6. (300-359s) Every second 3.5 new simulated users arrive

__Note__: Each user corresponds to seven requests and a user may wait up to ten
seconds with the delays.

---

# Test Environment

All tests were conducted on a single Amazon EC2 m3-medium instance.

* 1 vCPU

* 3.75 GB RAM

The tests used the `Puma` application server (unless otherwise specified).

The `database_optimizations` branch of the demo app was used to run the tests:
[https://github.com/scalableinternetservices/demo/tree/database_optimizations](https://github.com/scalableinternetservices/demo/tree/database_optimizations)

---

# Single Thread/Process (Users)

.center[![Single Thread/Process Users](demo_single_users.png)]

---

# Single Thread/Process (Page Load)

.left-column20[
Decrease in performance around 60s (1.5 new users per second)

Mean duration's spike is around 200 seconds.
]
.right-column80[
.center[![Single Thread/Process Page Load](demo_single_page_load.png)]
]

---

# Four Processes (Users)

![Four Processes Users](demo_four_users.png)

---

# Four Processes (Page Load)

.left-column20[
Decrease in performance around 240s (3 new users per second)

Mean duration's spike is just below 18 seconds.
]
.right-column80[
![Four Processes Page Load](demo_four_load.png)
]

---

# Sixteen Processes (Users)

![Sixteen Processes Users](demo_sixteen_users.png)

---

# Sixteen Processes (Page Load)

.left-column20[
Decrease in performance around 240s (3 new users per second)

Mean duration's spike is just below 14 seconds.

Little improvement over 4 processes especially considering up to 4x memory
usage.
]
.right-column80[
![Sixteen Processes Page Load](demo_sixteen_load.png)
]

---

# Threads instead of processes?

> What do you think will happen?

---

# Four Threads (Users)

![Four Threads Users](demo_four_threads_users.png)

---

# Four Threads (Page Load)

.left-column20[
Still decrease in performance around 240s, but more stable until then.

Mean duration's spike is about 14 seconds.
]
.right-column80[
![Four Threads Page Load](demo_four_threads_load.png)
]

---

# 32 Threads (Users)

![32 Threads Users](demo_32_threads_users.png)

---

# 32 Threads (Page Load)

.left-column20[
Decrease in performance beginning around 300s (3.5 new users per second)

Mean duration's spike is under 2 seconds.
]
.right-column80[
![32 Threads Page Load](demo_32_threads_load.png)
]

---

# Digression: Ruby interpreters

There are different versions of the Ruby interpreter. Different workloads may
benefit from using different interpreters.

## MRI (Matz's Ruby Interpreter)

* The reference version

* Written in C

* Has a global interpreter lock (GIL) that prevents true thread-concurrency

## JRuby

* Written in Java

* Does not have GIL

---

# Ruby Application Server: Puma

- "Puma is a simple, fast, multi-threaded, and highly concurrent HTTP 1.1 server for Ruby/Rack applications."

- "Puma ... serves the request using a thread pool. Each request is served in a separate thread, so truly concurrent Ruby implementations (JRuby, Rubinius) will use all available CPU cores."

- "On MRI, there is a Global VM Lock (GVL) that ensures only one thread can run Ruby code at a time. But if you're doing a lot of blocking IO (such as HTTP calls to external APIs like Twitter), Puma still improves MRI's throughput by allowing IO waiting to be done in parallel."

- "Puma also offers 'clustered mode'. Clustered mode forks workers from a master process."

- "In clustered mode, Puma can 'preload' your application. This loads all the application code prior to forking."

> Why might you want to wait to load the application prior to forking?

<https://github.com/puma/puma>

---

# Thread Safety Note

If you can use thread-parallelism, do it! But, making your code thread safe
isn't always obvious.

Things to consider:

* Your code

* Your code's many dependencies

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"
            type="text/javascript"></script>
    <script type="text/javascript">
      var hljs = remark.highlighter.engine;
      var slideshow = remark.create({
          highlightLanguage: 'ruby',
          highlightLines: true,
          hightlightSpans: true,
          highlightStyle: 'monokai'
        }) ;
    </script>
  </body>
</html>
